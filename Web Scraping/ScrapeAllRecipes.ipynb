{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad7993-9757-4e4b-8234-944f95b42fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "with urllib.request.urlopen('https://www.allrecipes.com/recipes/?page=2') as f:\n",
    "    print(f.read(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35133e26-a274-4d55-b805-3b4dccae77cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 2 scraping started!!!\n",
      "Page 2 Scraped!!!\n",
      "Page 3 scraping started!!!\n",
      "Page 3 Scraped!!!\n",
      "Page 4 scraping started!!!\n",
      "Page 4 Scraped!!!\n",
      "Page 5 scraping started!!!\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:/Datasets/dataset_ar.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-6edcf2b033e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;31m#print(dataset_ar)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/Datasets/dataset_ar.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Page \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" Scraped!!!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python391\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3384\u001b[0m         )\n\u001b[0;32m   3385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3386\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3387\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3388\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python391\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python391\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python391\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:/Datasets/dataset_ar.csv'"
     ]
    }
   ],
   "source": [
    "# Scraping Recipes - All Recipes\n",
    "import html\n",
    "from urllib.request import Request,urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "import os\n",
    "from sys import platform\n",
    "import sqlite3\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "base_url = 'https://www.allrecipes.com/recipes/236/us-recipes/?page='\n",
    "\n",
    "site_base = \"https://www.allrecipes.com\"\n",
    "\n",
    "df=pd.DataFrame()\n",
    "\n",
    "#conn = sqlite3.connect(r'D:/cuisine/usa/usa.db')\n",
    "# print(\"Opened database successfully\")\n",
    "#conn.execute('CREATE TABLE IF NOT EXISTS Recipes(Id Int,title Varchar,ingredients Varchar,instructions Varchar,picture_link Varchar);')\n",
    "# conn.execute(\"DROP TABLE Recipes\")\n",
    "#conn.close()\n",
    "\n",
    "\n",
    "recipe_count = 1\n",
    "\n",
    "for i in range(2, 50):# 101):\n",
    "    \n",
    "    print(\"Page \"+str(i)+\" scraping started!!!\")\n",
    "    \n",
    "    url = base_url + str(i)\n",
    "    req = Request(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    webpage = urlopen(req).read()\n",
    "    page_soup = soup(webpage,\"html.parser\")\n",
    "    \n",
    "    recipe_links_on_page = []\n",
    "    \n",
    "    page_soup = page_soup.find('main')\n",
    "    links = page_soup.find_all(\"a\", class_=\"tout__titleLink\")\n",
    "   \n",
    "    for link in links:\n",
    "        if link[\"href\"].startswith(\"/recipe/\"):\n",
    "            recipe_links_on_page.append(site_base + link[\"href\"])\n",
    "\n",
    "        \n",
    "    for recipe_url in recipe_links_on_page:\n",
    "        req_recipe = Request(recipe_url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "        webpage_recipe = urlopen(req_recipe).read()\n",
    "        page_soup_recipe = soup(webpage_recipe,\"html.parser\")\n",
    "        \n",
    "        # Name of Recipe\n",
    "        #print()\n",
    "        name = page_soup_recipe.find('h1').text\n",
    "        #page_soup_recipe.find('h1',class_='headline heading-content')\n",
    "        #print(name)\n",
    "        \n",
    "        # Ingredients\n",
    "        ingredients = page_soup_recipe.find_all('span',class_='ingredients-item-name')\n",
    "        temp_ingredients = []\n",
    "        \n",
    "        for ingredient in ingredients:\n",
    "            temp_ingredients.append(ingredient.text.strip())\n",
    "        \n",
    "        # Recipe \n",
    "        recipes = page_soup_recipe.find(\"ul\", { \"class\" : \"instructions-section\" }).findAll(\"li\", recursive=False)\n",
    "        temp_recipe = []\n",
    "        \n",
    "        for x in range(len(recipes)):\n",
    "            temp_recipe.append(recipes[x].find('p').text)\n",
    "    \n",
    "        # Image\n",
    "        temp_img = page_soup_recipe.find(\"div\", { \"class\" : \"docked-sharebar-content-container\" })\n",
    "        \n",
    "        #print(\"temp1\")\n",
    "        #print(temp_img)\n",
    "\n",
    "        temp_img = temp_img.find(\"div\",{\"class\" : \"image-filmstrip\"})\n",
    "        #print(\"temp2\")\n",
    "        #print(temp_img)\n",
    "        if temp_img.find(\"div\") == None:\n",
    "            continue\n",
    "        \n",
    "        #temp_img = temp_img.find(\"img\")\n",
    "        #print(temp_img)\n",
    "        \n",
    "        if temp_img.find(\"img\") == None:\n",
    "            continue\n",
    "        \n",
    "        #print(temp_img.find(\"img\"))\n",
    "        temp_img = temp_img.find(\"img\")[\"src\"]\n",
    "        \n",
    "        #print(temp_img)\n",
    "        img_name = r\"D:/Datasets/\" + str(recipe_count) + '.jpg'\n",
    "        urllib.request.urlretrieve(str(temp_img), img_name)\n",
    "\n",
    "        #print(\"i am here\")\n",
    "        # Data to be written \n",
    "        dictionary ={ \n",
    "            \"id\" : recipe_count,\n",
    "            \"title\" : name, \n",
    "            \"ingredients\" : temp_ingredients, \n",
    "            \"instructions\" : temp_recipe, \n",
    "            \"picture_link\" : temp_img\n",
    "        }\n",
    "        #json_object = json.dumps(dictionary, indent = 4) \n",
    "        \n",
    "        df=df.append(dictionary,ignore_index=True)\n",
    "        #print(dictionary)\n",
    "        # Wdriting to dataset_ar.json \n",
    "        #with open(\"dataset_ar.json\", \"w\") as outfile: \n",
    "         #       outfile.write(json_object)\n",
    "        \n",
    "        \n",
    "        # Done with the recipe\n",
    "        #print(\"Recipe \"+str(recipe_count)+\" Scrapped\")\n",
    "        recipe_count += 1\n",
    "        \n",
    "   \n",
    "\n",
    "   # df=pd.DataFrame.from_dict(dictionary)\n",
    "    #print(df)\n",
    "    # Storing in CSV File\n",
    "    #dataset_ar = pd.DataFrame(list(zip(recipe_count,name, temp_ingredients,temp_ingredients,temp_recipe,temp_img)), \n",
    "    #columns =['id','title', 'ingredients', 'instructions', 'picture_link'])\n",
    "\n",
    "    #print(dataset_ar)\n",
    "    df.to_csv('D:/Datasets/dataset_ar.csv')        \n",
    "    print(\"Page \"+str(i)+\" Scraped!!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3df873-cc28-4f36-ad0e-5e499a1dbcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a1cef-29ef-49ff-b03c-6aa762bbbbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
